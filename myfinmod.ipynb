{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de255e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd  # 用于数据处理和分析\n",
    "import numpy as np  # 用于数值计算\n",
    "import matplotlib.pyplot as plt  # 用于数据可视化\n",
    "import seaborn as sns  # 用于高级数据可视化\n",
    "import os  # 用于操作系统相关功能，如文件路径操作\n",
    "import datetime  # 用于处理日期和时间\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler  # 用于数据预处理（标准化、编码等）\n",
    "from sklearn.impute import SimpleImputer  # 用于处理缺失值\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score  # 用于数据集划分和交叉验证\n",
    "from sklearn.metrics import roc_auc_score, roc_curve  # 用于评估模型性能（AUC指标）\n",
    "import xgboost as xgb  # XGBoost模型库\n",
    "import lightgbm as lgb  # LightGBM模型库\n",
    "from catboost import CatBoostClassifier  # CatBoost模型库\n",
    "from sklearn.linear_model import LogisticRegression  # 用于逻辑回归模型\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier  # 用于集成学习模型\n",
    "import warnings  # 用于忽略警告信息\n",
    "warnings.filterwarnings('ignore')  # 忽略所有警告信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2901105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，保证结果可复现\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d85e7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):#把数据类型转换为更小的类型\n",
    "    \"\"\"\n",
    "    减少内存使用的函数\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('初始内存占用: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('优化后内存占用: {:.2f} MB'.format(end_mem))\n",
    "    print('内存减少了 {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123b4b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "初始内存占用: 286.87 MB\n",
      "优化后内存占用: 69.46 MB\n",
      "内存减少了 75.8%\n",
      "初始内存占用: 70.19 MB\n",
      "优化后内存占用: 17.19 MB\n",
      "内存减少了 75.5%\n",
      "训练集大小: (800000, 47)\n",
      "测试集大小: (200000, 46)\n"
     ]
    }
   ],
   "source": [
    "# ===================== 数据集简易处理 ===================== #\n",
    "# 数据读取\n",
    "print('读取数据...')\n",
    "train_data = pd.read_csv(r\"C:\\Users\\16050\\Desktop\\train.csv\")\n",
    "test_data = pd.read_csv(r\"C:\\Users\\16050\\Desktop\\testA.csv\")\n",
    "# 读取提交文件\n",
    "if os.path.exists('submission.csv'):\n",
    "    submission = pd.read_csv('submission.csv')\n",
    "else:\n",
    "    submission = pd.DataFrame({'id': test_data['id'], 'isDefault': 0})\n",
    "\n",
    "# 减少训练集、测试集内存占用\n",
    "train_data = reduce_mem_usage(train_data)\n",
    "test_data = reduce_mem_usage(test_data)\n",
    "\n",
    "# 显示数据集大小\n",
    "print(f'训练集大小: {train_data.shape}')\n",
    "print(f'测试集大小: {test_data.shape}')\n",
    "\n",
    "# 合并数据集，方便同时进行数据处理\n",
    "target = 'isDefault'  # 目标变量名 是否违约\n",
    "train_target = train_data[target]\n",
    "#保存id\n",
    "train_id = train_data['id']\n",
    "test_id = test_data['id']\n",
    "\n",
    "# 删除ID列，因为id列仅用于标识，不参与建模，所以需要删除\n",
    "train_data = train_data.drop(['id', target], axis=1)\n",
    "test_data = test_data.drop(['id'], axis=1)\n",
    "\n",
    "# 合并数据，便于后续处理\n",
    "data_all = pd.concat([train_data, test_data], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c1e0e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进行数据探索...\n",
      "数据基本信息:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 45 columns):\n",
      " #   Column              Non-Null Count    Dtype   \n",
      "---  ------              --------------    -----   \n",
      " 0   loanAmnt            1000000 non-null  float16 \n",
      " 1   term                1000000 non-null  int8    \n",
      " 2   interestRate        1000000 non-null  float16 \n",
      " 3   installment         1000000 non-null  float16 \n",
      " 4   grade               1000000 non-null  category\n",
      " 5   subGrade            1000000 non-null  category\n",
      " 6   employmentTitle     999999 non-null   float32 \n",
      " 7   employmentLength    941459 non-null   category\n",
      " 8   homeOwnership       1000000 non-null  int8    \n",
      " 9   annualIncome        1000000 non-null  float32 \n",
      " 10  verificationStatus  1000000 non-null  int8    \n",
      " 11  issueDate           1000000 non-null  object  \n",
      " 12  purpose             1000000 non-null  int8    \n",
      " 13  postCode            999999 non-null   float16 \n",
      " 14  regionCode          1000000 non-null  int8    \n",
      " 15  dti                 999700 non-null   float16 \n",
      " 16  delinquency_2years  1000000 non-null  float16 \n",
      " 17  ficoRangeLow        1000000 non-null  float16 \n",
      " 18  ficoRangeHigh       1000000 non-null  float16 \n",
      " 19  openAcc             1000000 non-null  float16 \n",
      " 20  pubRec              1000000 non-null  float16 \n",
      " 21  pubRecBankruptcies  999479 non-null   float16 \n",
      " 22  revolBal            1000000 non-null  float32 \n",
      " 23  revolUtil           999342 non-null   float16 \n",
      " 24  totalAcc            1000000 non-null  float16 \n",
      " 25  initialListStatus   1000000 non-null  int8    \n",
      " 26  applicationType     1000000 non-null  int8    \n",
      " 27  earliesCreditLine   1000000 non-null  object  \n",
      " 28  title               999999 non-null   float16 \n",
      " 29  policyCode          1000000 non-null  float16 \n",
      " 30  n0                  949619 non-null   float16 \n",
      " 31  n1                  949619 non-null   float16 \n",
      " 32  n2                  949619 non-null   float16 \n",
      " 33  n3                  949619 non-null   float16 \n",
      " 34  n4                  958367 non-null   float16 \n",
      " 35  n5                  949619 non-null   float16 \n",
      " 36  n6                  949619 non-null   float16 \n",
      " 37  n7                  949619 non-null   float16 \n",
      " 38  n8                  949618 non-null   float16 \n",
      " 39  n9                  949619 non-null   float16 \n",
      " 40  n10                 958367 non-null   float16 \n",
      " 41  n11                 912673 non-null   float16 \n",
      " 42  n12                 949619 non-null   float16 \n",
      " 43  n13                 949619 non-null   float16 \n",
      " 44  n14                 949619 non-null   float16 \n",
      "dtypes: category(3), float16(30), float32(3), int8(7), object(2)\n",
      "memory usage: 93.5+ MB\n",
      "None\n",
      "数值型特征数量: 40\n",
      "类别型特征数量: 5\n",
      "包含缺失值的列: ['employmentTitle', 'employmentLength', 'postCode', 'dti', 'pubRecBankruptcies', 'revolUtil', 'title', 'n0', 'n1', 'n2', 'n3', 'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n11', 'n12', 'n13', 'n14']\n",
      "包含缺失值的列数量: 22\n"
     ]
    }
   ],
   "source": [
    "# ===================== 数据探索 ===================== #\n",
    "print('进行数据探索...')\n",
    "\n",
    "# 显示基本信息\n",
    "print('数据基本信息:')\n",
    "print(data_all.info())\n",
    "\n",
    "# 数值型变量分析\n",
    "numerical_fea = list(data_all.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns)\n",
    "category_fea = list(filter(lambda x: x not in numerical_fea, list(data_all.columns)))\n",
    "print(f'数值型特征数量: {len(numerical_fea)}')\n",
    "\n",
    "# 类别型变量分析 上面那个是排除法，这个是包含法\n",
    "# 这里将object和category类型的特征都视为类别型特征\n",
    "category_fea = list(data_all.select_dtypes(include=['object', 'category']).columns)\n",
    "print(f'类别型特征数量: {len(category_fea)}')\n",
    "\n",
    "# 查看缺失值情况\n",
    "missing_data = data_all.isnull().sum()\n",
    "missing_cols = missing_data[missing_data > 0].index.tolist()\n",
    "print(f'包含缺失值的列: {missing_cols}')\n",
    "print(f'包含缺失值的列数量: {len(missing_cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080fa489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进行特征工程...\n"
     ]
    }
   ],
   "source": [
    "# ===================== 特征工程 ===================== #\n",
    "print('进行特征工程...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36d247",
   "metadata": {},
   "source": [
    "# 这个地方将连续变量离散化为5个离散变量，也许可以修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc37896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 时间特征处理 --------\n",
    "# 将issueDate转换为时间格式并提取更多时间特征\n",
    "for data in [data_all]:\n",
    "    # 转换时间格式，将字符串转换为日期时间对象\n",
    "    data['issueDate'] = pd.to_datetime(data['issueDate'], format='%Y-%m-%d')\n",
    "    \n",
    "    # 提取基本时间特征，包括年、月、日、星期几、季度等\n",
    "    data['issueDateYear'] = data['issueDate'].dt.year\n",
    "    data['issueDateMonth'] = data['issueDate'].dt.month\n",
    "    data['issueDateDay'] = data['issueDate'].dt.day\n",
    "    data['issueDateWeekday'] = data['issueDate'].dt.weekday\n",
    "    data['issueDateQuarter'] = data['issueDate'].dt.quarter\n",
    "    \n",
    "    # 计算相对日期（距离2007-06-01的天数）\n",
    "    startdate = datetime.datetime.strptime('2007-06-01', '%Y-%m-%d')\n",
    "    data['issueDateDT'] = data['issueDate'].apply(lambda x: x-startdate).dt.days\n",
    "\n",
    "# 处理earliesCreditLine特征，最早开通账户时间\n",
    "data_all['earliesCreditLine'] = pd.to_datetime(data_all['earliesCreditLine'], format='%b-%Y', errors='coerce')\n",
    "\n",
    "# 创建信用历史年龄特征，单位是月份\n",
    "data_all['creditAge'] = (data_all['issueDate'].dt.year - data_all['earliesCreditLine'].dt.year) * 12 + \\\n",
    "                       (data_all['issueDate'].dt.month - data_all['earliesCreditLine'].dt.month)\n",
    "\n",
    "# 提取earliesCreditLine的月份和年份作为特征\n",
    "data_all['earliesCreditLineYear'] = data_all['earliesCreditLine'].dt.year\n",
    "data_all['earliesCreditLineMonth'] = data_all['earliesCreditLine'].dt.month\n",
    "\n",
    "# 计算信用年龄分类，分为五类，连续变量离散化\n",
    "data_all['creditAgeBin'] = pd.qcut(data_all['creditAge'].clip(lower=0), 5, labels=False, duplicates='drop')\n",
    "\n",
    "# 删除原始日期列，只保留处理后的特征\n",
    "data_all.drop(['issueDate', 'earliesCreditLine'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f40b2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 缺失值处理 --------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9223da5",
   "metadata": {},
   "source": [
    "# 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268371a5",
   "metadata": {},
   "source": [
    "# ====== 处理employmentLength特征 ======\n",
    "# 步骤1：统一转换为字符串类型并处理缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0230a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['employmentLength'] = data_all['employmentLength'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b696fa",
   "metadata": {},
   "source": [
    "# 步骤2：标准化字符串格式（处理所有可能的格式变化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e9b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all['employmentLength'] = (\n",
    "    data_all['employmentLength']\n",
    "    .str.strip()       # 第一步：去除首尾空格\n",
    "    .str.lower()       # 第二步：统一小写\n",
    "    .replace({         # 第三步：替换不规则字符串\n",
    "        '< 1 year': '0',         # 小于1年 → 0\n",
    "        '10+ years': '10',       # 10年以上 → 10\n",
    "        'nan': np.nan,           # 字符串 'nan' 转成实际的缺失值\n",
    "        'n/a': np.nan,           # 表示空的、不可用的也设为缺失\n",
    "        '': np.nan               # 空字符串也设为缺失\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "279ca0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤3：安全提取数值\n",
    "def safe_extract(value):\n",
    "    try:\n",
    "        # 首先检查value是否为nan\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "            \n",
    "        # 如果已经是数字类型，直接返回\n",
    "        if isinstance(value, (int, float)):\n",
    "            return value\n",
    "            \n",
    "        # 处理浮点型字符串（如\"5.0\"）\n",
    "        if isinstance(value, str) and '.' in value:\n",
    "            return float(value.split()[0])\n",
    "            \n",
    "        # 处理整数型字符串（如\"5 years\"）\n",
    "        if isinstance(value, str):\n",
    "            return int(value.split()[0])\n",
    "            \n",
    "        return np.nan\n",
    "    except (ValueError, AttributeError, IndexError, TypeError):\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2963f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_all['employmentLength'] = data_all['employmentLength'].apply(safe_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414060be",
   "metadata": {},
   "source": [
    "# 这里填充缺失值的方法是不是可以考虑修改一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5097f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步骤4：中位数填充缺失值\n",
    "median_value = data_all['employmentLength'].median()\n",
    "data_all['employmentLength'] = data_all['employmentLength'].fillna(median_value).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "595b6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employmentLength处理后取值分布:\n",
      "employmentLength\n",
      "10    328525\n",
      "6     105123\n",
      "2      90565\n",
      "0      80226\n",
      "3      80163\n",
      "1      65671\n",
      "5      62645\n",
      "4      59818\n",
      "8      45168\n",
      "7      44230\n",
      "9      37866\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 步骤5：验证处理结果\n",
    "print(\"employmentLength处理后取值分布:\")\n",
    "print(data_all['employmentLength'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78565e26",
   "metadata": {},
   "source": [
    "# 这里异常值处理和缺失值填充的方法是否可以修改呢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7fef66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列 loanAmnt 的异常值比例: 0.0000\n",
      "列 loanAmnt 使用3sigma法则处理异常值\n",
      "列 term 的异常值比例: 0.0000\n",
      "列 term 使用3sigma法则处理异常值\n",
      "列 interestRate 的异常值比例: 0.0000\n",
      "列 interestRate 使用3sigma法则处理异常值\n",
      "列 installment 的异常值比例: 0.0000\n",
      "列 installment 使用3sigma法则处理异常值\n",
      "列 employmentTitle 的异常值比例: 0.0000\n",
      "列 employmentTitle 使用3sigma法则处理异常值\n",
      "列 homeOwnership 的异常值比例: 0.0004\n",
      "列 homeOwnership 使用3sigma法则处理异常值\n",
      "列 annualIncome 的异常值比例: 0.0073\n",
      "列 annualIncome 使用3sigma法则处理异常值\n",
      "列 verificationStatus 的异常值比例: 0.0000\n",
      "列 verificationStatus 使用3sigma法则处理异常值\n",
      "列 purpose 的异常值比例: 0.0213\n",
      "列 purpose 使用四分位法处理异常值\n",
      "列 postCode 的异常值比例: 0.0000\n",
      "列 postCode 使用3sigma法则处理异常值\n",
      "列 regionCode 的异常值比例: 0.0000\n",
      "列 regionCode 使用3sigma法则处理异常值\n",
      "列 dti 的异常值比例: 0.0000\n",
      "列 dti 使用3sigma法则处理异常值\n",
      "列 delinquency_2years 的异常值比例: 0.0000\n",
      "列 delinquency_2years 使用3sigma法则处理异常值\n",
      "列 ficoRangeLow 的异常值比例: 0.0000\n",
      "列 ficoRangeLow 使用3sigma法则处理异常值\n",
      "列 ficoRangeHigh 的异常值比例: 0.0000\n",
      "列 ficoRangeHigh 使用3sigma法则处理异常值\n",
      "列 openAcc 的异常值比例: 0.0000\n",
      "列 openAcc 使用3sigma法则处理异常值\n",
      "列 pubRec 的异常值比例: 0.0000\n",
      "列 pubRec 使用3sigma法则处理异常值\n",
      "列 pubRecBankruptcies 的异常值比例: 0.0000\n",
      "列 pubRecBankruptcies 使用3sigma法则处理异常值\n",
      "列 revolBal 的异常值比例: 0.0125\n",
      "列 revolBal 使用四分位法处理异常值\n",
      "列 revolUtil 的异常值比例: 0.0000\n",
      "列 revolUtil 使用3sigma法则处理异常值\n",
      "列 totalAcc 的异常值比例: 0.0000\n",
      "列 totalAcc 使用3sigma法则处理异常值\n",
      "列 initialListStatus 的异常值比例: 0.0000\n",
      "列 initialListStatus 使用3sigma法则处理异常值\n",
      "列 applicationType 的异常值比例: 0.0193\n",
      "列 applicationType 使用四分位法处理异常值\n",
      "列 title 的异常值比例: 0.0000\n",
      "列 title 使用3sigma法则处理异常值\n",
      "列 policyCode 的异常值比例: 0.0000\n",
      "列 policyCode 使用3sigma法则处理异常值\n",
      "列 n0 的异常值比例: 0.0000\n",
      "列 n0 使用3sigma法则处理异常值\n",
      "列 n1 的异常值比例: 0.0000\n",
      "列 n1 使用3sigma法则处理异常值\n",
      "列 n2 的异常值比例: 0.0000\n",
      "列 n2 使用3sigma法则处理异常值\n",
      "列 n3 的异常值比例: 0.0000\n",
      "列 n3 使用3sigma法则处理异常值\n",
      "列 n4 的异常值比例: 0.0000\n",
      "列 n4 使用3sigma法则处理异常值\n",
      "列 n5 的异常值比例: 0.0000\n",
      "列 n5 使用3sigma法则处理异常值\n",
      "列 n6 的异常值比例: 0.0000\n",
      "列 n6 使用3sigma法则处理异常值\n",
      "列 n7 的异常值比例: 0.0000\n",
      "列 n7 使用3sigma法则处理异常值\n",
      "列 n8 的异常值比例: 0.0000\n",
      "列 n8 使用3sigma法则处理异常值\n",
      "列 n9 的异常值比例: 0.0000\n",
      "列 n9 使用3sigma法则处理异常值\n",
      "列 n10 的异常值比例: 0.0000\n",
      "列 n10 使用3sigma法则处理异常值\n",
      "列 n11 的异常值比例: 0.0007\n",
      "列 n11 使用3sigma法则处理异常值\n",
      "列 n12 的异常值比例: 0.0031\n",
      "列 n12 使用3sigma法则处理异常值\n",
      "列 n13 的异常值比例: 0.0000\n",
      "列 n13 使用3sigma法则处理异常值\n",
      "列 n14 的异常值比例: 0.0000\n",
      "列 n14 使用3sigma法则处理异常值\n"
     ]
    }
   ],
   "source": [
    "# 对数值型特征进行异常值处理和缺失值填充\n",
    "\n",
    "#异常值处理，使用3sigma法则和分位数法\n",
    "for col in numerical_fea:\n",
    "    if col in data_all.columns:\n",
    "        # 计算统计值用于异常值处理\n",
    "        mean_val = data_all[col].mean()\n",
    "        std_val = data_all[col].std()\n",
    "        median_val = data_all[col].median()\n",
    "        upper_limit = mean_val + 3 * std_val\n",
    "        lower_limit = mean_val - 3 * std_val\n",
    "        \n",
    "        # 记录异常值比例\n",
    "        outlier_ratio = ((data_all[col] > upper_limit) | (data_all[col] < lower_limit)).mean()\n",
    "        print(f\"列 {col} 的异常值比例: {outlier_ratio:.4f}\")\n",
    "        \n",
    "        # 对异常值进行处理：使用分位数法\n",
    "        if outlier_ratio > 0.01:  # 如果异常值比例大于1%\n",
    "            q1 = data_all[col].quantile(0.25)\n",
    "            q3 = data_all[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            \n",
    "            # 使用边界值替换异常值\n",
    "            data_all.loc[data_all[col] > upper_bound, col] = upper_bound\n",
    "            data_all.loc[data_all[col] < lower_bound, col] = lower_bound\n",
    "            print(f\"列 {col} 使用四分位法处理异常值\")\n",
    "        else:  # 如果异常值比例较小，使用3sigma法则\n",
    "            # 对异常值进行替换\n",
    "            data_all.loc[data_all[col] > upper_limit, col] = upper_limit\n",
    "            data_all.loc[data_all[col] < lower_limit, col] = lower_limit\n",
    "            print(f\"列 {col} 使用3sigma法则处理异常值\")\n",
    "        \n",
    "        # 缺失值填充，使用中位数填充\n",
    "        data_all[col] = data_all[col].fillna(median_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ead092",
   "metadata": {},
   "source": [
    "# 使用众数填充真的好吗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b79fcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列 grade 使用众数 B 填充缺失值\n",
      "列 subGrade 使用众数 C1 填充缺失值\n",
      "列 employmentLength 使用众数 10 填充缺失值\n"
     ]
    }
   ],
   "source": [
    "# 对类别型特征进行缺失值填充\n",
    "for col in category_fea:\n",
    "    if col in data_all.columns:\n",
    "        # 使用众数填充\n",
    "        mode_val = data_all[col].mode()[0]\n",
    "        data_all[col] = data_all[col].fillna(mode_val)\n",
    "        print(f\"列 {col} 使用众数 {mode_val} 填充缺失值\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7096aa",
   "metadata": {},
   "source": [
    "# 这个编码方法没听说过，不知道好用不好用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c578f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 特征编码 --------\n",
    "# 对类别型特征进行编码成数字，使用LabelEncoder，方便后续模型训练\n",
    "for col in ['grade', 'subGrade', 'homeOwnership', 'verificationStatus', 'purpose', 'initialListStatus']:\n",
    "    if col in data_all.columns:\n",
    "        le = LabelEncoder()\n",
    "        data_all[col] = le.fit_transform(data_all[col].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2d89c",
   "metadata": {},
   "source": [
    "这段代码是在进行非常重要的一步：**特征衍生与交互特征构造**。\n",
    "\n",
    "通俗讲，它就是在“原始数据”基础上，**组合计算出新的特征变量**，以便让模型学到更多“潜在模式”，提高预测准确率。\n",
    "\n",
    "---\n",
    "\n",
    "我们来逐段解释：👇\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **目的是什么？**\n",
    "\n",
    "> **通过组合原始变量** → 构造出更具解释力的新变量（叫做“衍生特征”、“交互特征”）  \n",
    "> 让模型更容易捕捉复杂的非线性关系，提高模型性能。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 1~16：数学衍生特征\n",
    "\n",
    "每一行都是计算新特征变量：\n",
    "\n",
    "| 特征名 | 含义 | 建模价值 |\n",
    "|--------|------|-----------|\n",
    "| `debt_to_income` | `dti` 的拷贝 | 有时方便统一使用 |\n",
    "| `debt_to_income_ratio` | 负债比 / 月收入 | 月还款压力 |\n",
    "| `installment_income_ratio` | 月供 / 月收入 | 还款能力指标 |\n",
    "| `installment_income_ratio_yearly` | 年供 / 年收入 | 另一种还款能力表示 |\n",
    "| `loan_income_ratio` | 贷款金额 / 年收入 | 财务负担 |\n",
    "| `historical_delinquency_rate` | 逾期次数 / 账户数 | 信用表现相对值 |\n",
    "| `credit_utilization_ratio` | 已用额度 / 信用额度（+1防除0） | 信用卡使用比例 |\n",
    "| `interest_to_grade` | 利率 / 等级 | 反映风险收益对应性 |\n",
    "| `util_to_loan` | 信用额度利用率 × 贷款金额 | 复合风险特征 |\n",
    "| `installment_to_loan` | 每月供款 / 总贷款 | 是否选择了高月供短期贷 |\n",
    "| `term_to_interest` | 贷款期数 × 利率 | 总利息的线索 |\n",
    "| `loan_to_term` | 总贷款 / 期数 | 每期平均额度 |\n",
    "| `fico_to_interest` | 信用分 / 利率 | 是否与信用匹配 |\n",
    "| `open_to_total_acc` | 开放账户 / 总账户 | 账户活跃度 |\n",
    "| `fico_range_diff` | 最高分 - 最低分 | 信用分数波动情况 |\n",
    "| `loan_to_fico` | 贷款 / 信用评分 | 是否贷款额度过高 |\n",
    "| `acc_to_credit_age` | 账户数 / 信用龄 | 开户频率 |\n",
    "| `loan_to_installment` | 总贷款 / 每月供款 | 贷款周期隐含特征 |\n",
    "\n",
    "这些变量都能提供模型新的“角度”去观察违约风险。\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 17. **均值编码（Mean Encoding / Target Encoding）**\n",
    "\n",
    "```python\n",
    "for col in ['grade', 'subGrade', ...]:\n",
    "    encoding_dict = train_with_target.groupby(col)['isDefault'].mean().to_dict()\n",
    "    data_all[f'{col}_mean_target'] = data_all[col].map(encoding_dict)\n",
    "```\n",
    "\n",
    "- 逻辑：对于每个类别变量，计算该类别的平均违约率作为一个新的数值特征。\n",
    "- 例如：\n",
    "  - `grade = A` → 平均违约率为 0.05\n",
    "  - `grade = G` → 平均违约率为 0.23\n",
    "- **优势**：比 one-hot 编码更紧凑，保留了目标变量的信息\n",
    "\n",
    "⚠️：此处是训练集 `train_with_target` 上提取均值，确保无数据泄露。\n",
    "\n",
    "---\n",
    "\n",
    "## 🌍 18. 与地区相关的统计特征\n",
    "\n",
    "如果数据有地区（`regionCode`）：\n",
    "- 计算该地区的平均违约率\n",
    "- 计算该地区的平均贷款金额\n",
    "\n",
    "这些特征揭示了地区经济水平或信用行为的差异。\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 19. 与贷款用途相关的统计特征\n",
    "\n",
    "如果有贷款用途（`purpose`）：\n",
    "- 每种用途的平均利率\n",
    "- 每种用途的平均贷款金额\n",
    "- 每种用途的平均违约率\n",
    "\n",
    "有助于模型理解不同贷款动机的风险差异：\n",
    "> 比如“买车”贷款比“债务整合”贷款更安全。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结（人话版）\n",
    "\n",
    "这段代码的本质是：\n",
    "> 用数学计算、变量组合、分组统计等方法，**衍生出几十个新变量**，让模型不只是看“原始字段”，而是能学到“更深层的信息”。\n",
    "\n",
    "| 特征类型 | 方法 | 意图 |\n",
    "|----------|------|------|\n",
    "| 数学比率 | 除法、乘法 | 转化为相对值，更具稳定性 |\n",
    "| 交叉组合 | 变量相乘/相除 | 捕捉非线性关联 |\n",
    "| 均值编码 | groupby + mean | 提供类别的目标值倾向 |\n",
    "| 地区/用途统计 | 分组聚合 | 加入“群体行为”信号 |\n",
    "\n",
    "---\n",
    "\n",
    "如果你希望我：\n",
    "- 可视化这些衍生特征的分布\n",
    "- 检查这些特征和目标变量的相关性（信息增益、IV值、皮尔森系数）\n",
    "- 封装一个函数批量做这些事\n",
    "\n",
    "我可以直接帮你写 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b34728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 特征交互与衍生 --------\n",
    "# 1. 负债收入比的衍生特征\n",
    "data_all['debt_to_income'] = data_all['dti']\n",
    "data_all['debt_to_income_ratio'] = data_all['dti'] / (data_all['annualIncome'] / 12)\n",
    "\n",
    "# 2. 添加还款压力比例 (分期付款金额 / 年收入)\n",
    "data_all['installment_income_ratio'] = data_all['installment'] / (data_all['annualIncome'] / 12)\n",
    "data_all['installment_income_ratio_yearly'] = data_all['installment'] * 12 / data_all['annualIncome']\n",
    "\n",
    "# 3. 添加贷款金额与年收入的比例\n",
    "data_all['loan_income_ratio'] = data_all['loanAmnt'] / data_all['annualIncome']\n",
    "\n",
    "# 4. 添加历史逾期率\n",
    "data_all['historical_delinquency_rate'] = data_all['delinquency_2years'] / (data_all['openAcc'] + 1)\n",
    "\n",
    "# 5. 添加信用卡使用率特征\n",
    "data_all['credit_utilization_ratio'] = data_all['revolBal'] / (data_all['revolUtil'] + 1)\n",
    "\n",
    "# 6. 贷款利率与等级的交互\n",
    "data_all['interest_to_grade'] = data_all['interestRate'] / (data_all['grade'] + 1)\n",
    "\n",
    "# 7. 额度利用率与贷款金额的交互\n",
    "data_all['util_to_loan'] = data_all['revolUtil'] * data_all['loanAmnt'] / 10000\n",
    "\n",
    "# 8. 分期付款与贷款金额的比率\n",
    "data_all['installment_to_loan'] = data_all['installment'] / data_all['loanAmnt']\n",
    "\n",
    "# 9. 贷款期限与利率的交互\n",
    "data_all['term_to_interest'] = data_all['term'] * data_all['interestRate'] / 100\n",
    "\n",
    "# 10. 贷款金额与期限的比率\n",
    "data_all['loan_to_term'] = data_all['loanAmnt'] / data_all['term']\n",
    "\n",
    "# 11. 信用评分与利率的交互\n",
    "data_all['fico_to_interest'] = data_all['ficoRangeLow'] / data_all['interestRate']\n",
    "\n",
    "# 12. 总账户数与开放账户数的比率\n",
    "data_all['open_to_total_acc'] = data_all['openAcc'] / (data_all['totalAcc'] + 1)\n",
    "\n",
    "# 13. 添加信用评分差异\n",
    "data_all['fico_range_diff'] = data_all['ficoRangeHigh'] - data_all['ficoRangeLow']\n",
    "\n",
    "# 14. 添加贷款额度与信用评分的交互\n",
    "data_all['loan_to_fico'] = data_all['loanAmnt'] / data_all['ficoRangeLow']\n",
    "\n",
    "# 15. 添加总账户数与信用年龄的比率\n",
    "data_all['acc_to_credit_age'] = data_all['totalAcc'] / (data_all['creditAge'] + 1)\n",
    "\n",
    "# 16. 添加贷款金额与分期付款金额的比率\n",
    "data_all['loan_to_installment'] = data_all['loanAmnt'] / data_all['installment']\n",
    "\n",
    "# 17. 添加均值编码特征\n",
    "# 确保训练数据中有isDefault列\n",
    "train_with_target = data_all[:len(train_data)].copy()\n",
    "train_with_target['isDefault'] = train_target.values\n",
    "\n",
    "for col in ['grade', 'subGrade', 'homeOwnership', 'verificationStatus', 'purpose', 'initialListStatus']:\n",
    "    # 对训练集计算每个类别的目标均值\n",
    "    if col in data_all.columns:\n",
    "        encoding_dict = train_with_target.groupby(col)['isDefault'].mean().to_dict()\n",
    "        data_all[f'{col}_mean_target'] = data_all[col].map(encoding_dict)\n",
    "\n",
    "# 18. 添加与地区相关的统计特征\n",
    "if 'regionCode' in data_all.columns:\n",
    "    # 每个regionCode的违约率\n",
    "    region_default = train_with_target.groupby('regionCode')['isDefault'].mean()\n",
    "    data_all['region_default_rate'] = data_all['regionCode'].map(region_default)\n",
    "    \n",
    "    # 每个regionCode的平均贷款金额\n",
    "    region_loan = data_all.groupby('regionCode')['loanAmnt'].mean()\n",
    "    data_all['region_avg_loan'] = data_all['regionCode'].map(region_loan)\n",
    "\n",
    "# 19. 添加贷款目的相关的统计特征\n",
    "if 'purpose' in data_all.columns:\n",
    "    # 每种目的的平均利率\n",
    "    purpose_interest = data_all.groupby('purpose')['interestRate'].mean()\n",
    "    data_all['purpose_avg_interest'] = data_all['purpose'].map(purpose_interest)\n",
    "    \n",
    "    # 每种目的的平均贷款金额\n",
    "    purpose_loan = data_all.groupby('purpose')['loanAmnt'].mean()\n",
    "    data_all['purpose_avg_loan'] = data_all['purpose'].map(purpose_loan)\n",
    "    \n",
    "    # 每种目的的违约率\n",
    "    purpose_default = train_with_target.groupby('purpose')['isDefault'].mean()\n",
    "    data_all['purpose_default_rate'] = data_all['purpose'].map(purpose_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890a8a8",
   "metadata": {},
   "source": [
    "# 计算特征值之间的相关系数，如果相关系数大，就移除一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc31338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== 特征选择 ===================== #\n",
    "# 移除高度相关特征\n",
    "\n",
    "def correlation_selector(X, threshold=0.8):\n",
    "    try:\n",
    "        corr_matrix = X.corr().abs()\n",
    "        # 确保使用numpy.triu正确创建三角矩阵掩码\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "        return to_drop\n",
    "    except Exception as e:\n",
    "        print(f\"相关性分析失败: {str(e)}\")\n",
    "        # 如果失败，返回空列表\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b19a48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离训练集和测试集\n",
    "train = data_all[:len(train_data)]\n",
    "test = data_all[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "505988bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高相关性特征数量: 23\n",
      "高相关性特征: ['installment', 'grade', 'subGrade', 'ficoRangeHigh', 'n3', 'n9', 'n10', 'issueDateQuarter', 'issueDateDT', 'earliesCreditLineYear', 'debt_to_income', 'installment_income_ratio_yearly', 'loan_income_ratio', 'historical_delinquency_rate', 'loan_to_term', 'loan_to_fico', 'loan_to_installment', 'grade_mean_target', 'subGrade_mean_target', 'verificationStatus_mean_target', 'initialListStatus_mean_target', 'purpose_avg_interest', 'purpose_default_rate']\n"
     ]
    }
   ],
   "source": [
    "# 检查高相关性特征\n",
    "try:\n",
    "    high_corr_features = correlation_selector(train.select_dtypes(include=['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']), 0.90)\n",
    "    print(f\"高相关性特征数量: {len(high_corr_features)}\")\n",
    "    print(f\"高相关性特征: {high_corr_features}\")\n",
    "    \n",
    "    # 去除高相关性特征\n",
    "    data_all = data_all.drop(high_corr_features, axis=1)\n",
    "except Exception as e:\n",
    "    print(f\"去除高相关性特征失败: {str(e)}\")\n",
    "    # 不做任何删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49eb173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新分离训练集和测试集\n",
    "train = data_all[:len(train_data)]\n",
    "test = data_all[len(train_data):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679fa492",
   "metadata": {},
   "source": [
    "# 对于非树模型，将数据进行标准化，标准化为均值为0，方差为1的正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9dda7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化数值特征 (仅针对非树模型)\n",
    "numeric_features_for_scaling = [col for col in numerical_fea if col in train.columns]\n",
    "scaler = StandardScaler()\n",
    "train_scaled = train.copy()\n",
    "test_scaled = test.copy()\n",
    "train_scaled[numeric_features_for_scaling] = scaler.fit_transform(train[numeric_features_for_scaling])\n",
    "test_scaled[numeric_features_for_scaling] = scaler.transform(test[numeric_features_for_scaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f5d9d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "开始模型训练与调参...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================== 模型训练与调参 ===================== #\n",
    "print('='*80)\n",
    "print('开始模型训练与调参...')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d5b09",
   "metadata": {},
   "source": [
    "这段代码是在进行模型训练前的一项关键操作：**数据集划分 + 特征值预处理（处理异常值、无穷值、缺失值）**。我们来一块块拆解讲清楚。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 第一部分：划分训练集和验证集\n",
    "```python\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train, train_target, test_size=0.2, random_state=42, stratify=train_target\n",
    ")\n",
    "```\n",
    "\n",
    "### ✅ 作用\n",
    "- 将你的训练数据（`train`）和目标标签（`train_target`）**划分为训练集和验证集**。\n",
    "- `test_size=0.2` 表示 20% 的数据将作为验证集。\n",
    "- `stratify=train_target` 表示使用**分层抽样**，保持训练集和验证集中类别分布一致（用于分类任务，防止类不平衡）。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 第二部分：处理无穷大值和异常值\n",
    "\n",
    "```python\n",
    "for col in X_train.columns:\n",
    "    ...\n",
    "```\n",
    "\n",
    "### 🧪 为什么这么做？\n",
    "\n",
    "模型训练前必须确保数据**干净合理**：\n",
    "- 没有 `inf`（无穷值）或 `nan`（空值）\n",
    "- 没有极端异常值（比如特征值是 `1e20` 或 `-1e20` 这种极大极小数）\n",
    "- 否则模型可能不收敛、预测不稳定，甚至直接报错\n",
    "\n",
    "---\n",
    "\n",
    "### 🧼 清洗步骤一：处理 `inf` 和 `-inf`\n",
    "```python\n",
    "X_train[col] = X_train[col].replace([np.inf, -np.inf], np.nan)\n",
    "```\n",
    "- 将所有正无穷和负无穷替换成 `np.nan`，为下一步的缺失值填充做准备。\n",
    "\n",
    "---\n",
    "\n",
    "### 🧼 清洗步骤二：处理异常大值\n",
    "```python\n",
    "upper_bound = X_train[col].quantile(0.99)\n",
    "lower_bound = X_train[col].quantile(0.01)\n",
    "\n",
    "if upper_bound > 1e8:\n",
    "    X_train[col] = X_train[col].clip(upper=upper_bound)\n",
    "```\n",
    "- 计算第1% 和 第99% 分位数（即保留中间98%的数据）\n",
    "- 如果第99%的值已经非常大（如大于1亿），说明该特征分布很偏，可能有极端异常值\n",
    "- 用 `clip()` 限制最大值，不让极端值影响模型训练\n",
    "\n",
    "---\n",
    "\n",
    "### 🧼 清洗步骤三：填充缺失值\n",
    "```python\n",
    "X_train[col] = X_train[col].fillna(X_train[col].median())\n",
    "```\n",
    "- 对缺失值 `NaN` 进行填充\n",
    "- 使用训练集的 **中位数**，这样在填充时不会泄露验证集或测试集的信息\n",
    "- 保持数据分布稳定，适合大多数模型\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结作用：\n",
    "\n",
    "| 步骤 | 操作 | 目的 |\n",
    "|------|------|------|\n",
    "| 1 | `train_test_split()` | 拆分训练集和验证集，确保泛化评估 |\n",
    "| 2 | `replace([inf, -inf], NaN)` | 清除非法值 |\n",
    "| 3 | `clip()` | 裁剪异常大值，防止模型受到极端干扰 |\n",
    "| 4 | `fillna()` | 填充缺失值，避免训练时报错 |\n",
    "\n",
    "---\n",
    "\n",
    "这段代码是非常“工程实践化”的处理方式。模型能不能训好，很多时候就看这些 **干净数据预处理环节**做得细不细了。\n",
    "\n",
    "---\n",
    "\n",
    "如果你希望我把这个过程**封装成函数**、或**做成自动化预处理模块**，或者帮你接入 `Pipeline`，我也可以帮你写出来～要不要？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cd3c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 划分训练集和验证集\n",
      "训练集大小: (640000, 59), 验证集大小: (160000, 59)\n",
      "\n",
      "2. 处理无穷大值和异常值\n",
      "数据预处理完成\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集和验证集\n",
    "print('1. 划分训练集和验证集')\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train, train_target, test_size=0.2, random_state=42, stratify=train_target\n",
    ")\n",
    "print(f'训练集大小: {X_train.shape}, 验证集大小: {X_val.shape}')\n",
    "\n",
    "# 处理无穷大值和过大值\n",
    "print('\\n2. 处理无穷大值和异常值')\n",
    "# 在模型训练前替换inf和极大值\n",
    "for col in X_train.columns:\n",
    "    # 替换无穷大值\n",
    "    X_train[col] = X_train[col].replace([np.inf, -np.inf], np.nan)\n",
    "    X_val[col] = X_val[col].replace([np.inf, -np.inf], np.nan)\n",
    "    test[col] = test[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # 检查并替换极大或极小的值\n",
    "    upper_bound = X_train[col].quantile(0.99)\n",
    "    lower_bound = X_train[col].quantile(0.01)\n",
    "    \n",
    "    # 如果上限非常大，用更合理的值替代\n",
    "    if upper_bound > 1e8:\n",
    "        X_train[col] = X_train[col].clip(upper=upper_bound)\n",
    "        X_val[col] = X_val[col].clip(upper=upper_bound)\n",
    "        test[col] = test[col].clip(upper=upper_bound)\n",
    "    \n",
    "    # 填充缺失值\n",
    "    X_train[col] = X_train[col].fillna(X_train[col].median())\n",
    "    X_val[col] = X_val[col].fillna(X_train[col].median())\n",
    "    test[col] = test[col].fillna(X_train[col].median())\n",
    "\n",
    "print('数据预处理完成')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfed90",
   "metadata": {},
   "source": [
    "# 这段代码定义了一个用于评估二分类模型性能的函数，它的核心是计算 AUC 值（ROC曲线下面积），这是衡量模型分类效果的一种常用且稳定的指标，尤其适用于类别不平衡的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cda0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义评估函数\n",
    "def model_evaluation(model, X, y):\n",
    "    y_pred_prob = model.predict_proba(X)[:, 1]\n",
    "    auc = roc_auc_score(y, y_pred_prob)\n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "245d5c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. 训练LightGBM模型...\n",
      "--------------------------------------------------\n",
      "开始训练LightGBM模型...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[992]\tvalid_0's auc: 0.735691\n",
      "LightGBM模型训练完成\n"
     ]
    }
   ],
   "source": [
    "# 模型1: LightGBM\n",
    "print('\\n3. 训练LightGBM模型...')\n",
    "print('-'*50)\n",
    "lgb_params = {\n",
    "    'objective': 'binary',              # 二分类任务\n",
    "    'boosting_type': 'gbdt',            # 使用梯度提升树（默认）\n",
    "    'metric': 'auc',                    # 使用 AUC 作为评估指标\n",
    "    'n_estimators': 1000,               # 最多训练1000棵树（但会早停）\n",
    "    'learning_rate': 0.05,              # 学习率（每棵树的权重缩小因子）\n",
    "    'max_depth': 7,                     # 每棵树的最大深度\n",
    "    'num_leaves': 31,                   # 每棵树的叶子节点数\n",
    "    'subsample': 0.8,                   # 每棵树训练时使用80%的样本（防过拟合）\n",
    "    'colsample_bytree': 0.8,            # 每棵树训练时使用80%的特征\n",
    "    'min_child_weight': 5,              # 控制叶子节点的最小样本权重和（防过拟合）\n",
    "    'min_child_samples': 30,            # 控制叶子节点的最小样本数\n",
    "    'min_split_gain': 0.01,             # 控制分裂的最小增益\n",
    "    'reg_alpha': 0.3,                   # L1 正则化\n",
    "    'reg_lambda': 0.3,                  # L2 正则化\n",
    "    'random_state': 42,                 # 固定随机种子，结果可复现\n",
    "    'n_jobs': -1,                       # 使用所有CPU核心\n",
    "    'verbosity': -1                     # 控制训练过程输出（-1表示静默）\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
    "# 最新的LightGBM版本中，早停参数通过callbacks参数传递\n",
    "print('开始训练LightGBM模型...')\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100)]  # 不再使用verbose参数\n",
    ")\n",
    "print('LightGBM模型训练完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb744473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM 前10个重要特征:\n",
      "                     feature  importance\n",
      "3            employmentTitle        1223\n",
      "40                 creditAge        1182\n",
      "44  installment_income_ratio        1170\n",
      "6               annualIncome        1148\n",
      "11                       dti        1113\n",
      "0                   loanAmnt        1051\n",
      "45  credit_utilization_ratio        1042\n",
      "18                 revolUtil         998\n",
      "51         open_to_total_acc         997\n",
      "9                   postCode         986\n"
     ]
    }
   ],
   "source": [
    "# 特征重要性\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nLightGBM 前10个重要特征:\")\n",
    "print(lgb_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3103e8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM验证集AUC: 0.7357\n"
     ]
    }
   ],
   "source": [
    "# 评估LightGBM模型\n",
    "lgb_auc = model_evaluation(lgb_model, X_val, y_val)\n",
    "print(f'LightGBM验证集AUC: {lgb_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "697cd633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. 训练XGBoost模型...\n",
      "--------------------------------------------------\n",
      "开始训练XGBoost模型...\n",
      "[0]\tvalidation_0-auc:0.70339\tvalidation_1-auc:0.70142\n",
      "[100]\tvalidation_0-auc:0.73731\tvalidation_1-auc:0.72935\n",
      "[200]\tvalidation_0-auc:0.74709\tvalidation_1-auc:0.73298\n",
      "[300]\tvalidation_0-auc:0.75424\tvalidation_1-auc:0.73456\n",
      "[400]\tvalidation_0-auc:0.76014\tvalidation_1-auc:0.73517\n",
      "[500]\tvalidation_0-auc:0.76547\tvalidation_1-auc:0.73561\n",
      "[600]\tvalidation_0-auc:0.77059\tvalidation_1-auc:0.73583\n",
      "[700]\tvalidation_0-auc:0.77547\tvalidation_1-auc:0.73603\n",
      "[800]\tvalidation_0-auc:0.78013\tvalidation_1-auc:0.73606\n",
      "[856]\tvalidation_0-auc:0.78264\tvalidation_1-auc:0.73593\n",
      "XGBoost模型训练完成\n"
     ]
    }
   ],
   "source": [
    "# 模型2: XGBoost\n",
    "print('\\n4. 训练XGBoost模型...')\n",
    "print('-'*50)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',      # 二分类任务，输出为概率\n",
    "    'learning_rate': 0.05,               # 学习率，控制每棵树对最终结果的影响\n",
    "    'max_depth': 6,                      # 决策树最大深度，限制模型复杂度\n",
    "    'min_child_weight': 3,               # 子节点所需的最小样本权重和，控制过拟合\n",
    "    'gamma': 0.1,                        # 分裂所需的最小损失下降，越大越保守\n",
    "    'subsample': 0.8,                    # 每棵树训练时随机采样的样本比例\n",
    "    'colsample_bytree': 0.8,             # 每棵树训练时随机采样的特征比例\n",
    "    'scale_pos_weight': 1,               # 类别不平衡时的正类权重，1 表示样本均衡\n",
    "    'reg_alpha': 0.1,                    # L1正则化项系数（控制特征选择）\n",
    "    'reg_lambda': 0.1,                   # L2正则化项系数（防止权重过大）\n",
    "    'n_estimators': 1000,                # 最多拟合的树数量（迭代次数上限）\n",
    "    'random_state': 42,                  # 随机种子，保证结果可复现\n",
    "    'n_jobs': -1,                        # 使用全部CPU核心进行训练\n",
    "    'eval_metric': 'auc',               # 使用AUC作为评估指标\n",
    "    'verbosity': 0,                      # 控制日志输出级别（0=只输出警告）\n",
    "    'early_stopping_rounds': 100,        # 如果验证集AUC 100轮未提升，则提前停止\n",
    "    'missing': np.nan                    # 指定缺失值的占位符（XGBoost自动处理）\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
    "\n",
    "print('开始训练XGBoost模型...')\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],  # 用训练集和验证集评估性能（训练集必须放前面以启用早停）\n",
    "    verbose=100  # 每100轮打印一次训练进度\n",
    ")\n",
    "\n",
    "print('XGBoost模型训练完成')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b336c899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost验证集AUC: 0.7361\n"
     ]
    }
   ],
   "source": [
    "# 评估XGBoost模型\n",
    "xgb_auc = model_evaluation(xgb_model, X_val, y_val)\n",
    "print(f'XGBoost验证集AUC: {xgb_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "994ba7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. 训练CatBoost模型...\n",
      "--------------------------------------------------\n",
      "开始训练CatBoost模型...\n",
      "0:\ttest: 0.6983230\tbest: 0.6983230 (0)\ttotal: 77.6ms\tremaining: 1m 17s\n",
      "100:\ttest: 0.7255135\tbest: 0.7255135 (100)\ttotal: 7.09s\tremaining: 1m 3s\n",
      "200:\ttest: 0.7300976\tbest: 0.7300976 (200)\ttotal: 13.4s\tremaining: 53.4s\n",
      "300:\ttest: 0.7320580\tbest: 0.7320580 (300)\ttotal: 20.2s\tremaining: 46.9s\n",
      "400:\ttest: 0.7332291\tbest: 0.7332291 (400)\ttotal: 27.1s\tremaining: 40.4s\n",
      "500:\ttest: 0.7340301\tbest: 0.7340301 (500)\ttotal: 33.3s\tremaining: 33.2s\n",
      "600:\ttest: 0.7346401\tbest: 0.7346401 (600)\ttotal: 39.8s\tremaining: 26.4s\n",
      "700:\ttest: 0.7350226\tbest: 0.7350226 (700)\ttotal: 46.7s\tremaining: 19.9s\n",
      "800:\ttest: 0.7354252\tbest: 0.7354288 (799)\ttotal: 53.4s\tremaining: 13.3s\n",
      "900:\ttest: 0.7356365\tbest: 0.7356365 (900)\ttotal: 59.8s\tremaining: 6.57s\n",
      "999:\ttest: 0.7358202\tbest: 0.7358311 (982)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7358310536\n",
      "bestIteration = 982\n",
      "\n",
      "Shrink model to first 983 iterations.\n",
      "CatBoost模型训练完成\n"
     ]
    }
   ],
   "source": [
    "# 模型3: CatBoost\n",
    "print('\\n5. 训练CatBoost模型...')\n",
    "print('-'*50)\n",
    "cat_params = {\n",
    "    'loss_function': 'Logloss',\n",
    "    'eval_metric': 'AUC',\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'min_child_samples': 20,\n",
    "    'l2_leaf_reg': 3,\n",
    "    'random_strength': 0.1,\n",
    "    'bagging_temperature': 1,\n",
    "    'iterations': 1000,\n",
    "    'random_seed': 42,\n",
    "    'thread_count': -1,\n",
    "    'verbose': 100  # 设置为100以每100次迭代显示一次进度\n",
    "}\n",
    "\n",
    "cat_model = CatBoostClassifier(**cat_params)\n",
    "print('开始训练CatBoost模型...')\n",
    "cat_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=100,  # 早停参数在fit方法中指定\n",
    "    use_best_model=True,  # 确保使用最佳模型\n",
    "    verbose=100  # 每100次迭代显示一次进度\n",
    ")\n",
    "print('CatBoost模型训练完成')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76cded67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost验证集AUC: 0.7358\n"
     ]
    }
   ],
   "source": [
    "# 评估CatBoost模型\n",
    "cat_auc = model_evaluation(cat_model, X_val, y_val)\n",
    "print(f'CatBoost验证集AUC: {cat_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cadf81cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. 训练随机森林模型...\n",
      "--------------------------------------------------\n",
      "开始训练随机森林模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   56.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机森林模型训练完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "# 模型4: 随机森林\n",
    "print('\\n6. 训练随机森林模型...')\n",
    "print('-'*50)\n",
    "rf_params = {\n",
    "    'n_estimators': 200,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 5,\n",
    "    'max_features': 'sqrt',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': 1  # 显示训练进度\n",
    "}\n",
    "\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "print('开始训练随机森林模型...')\n",
    "rf_model.fit(X_train, y_train)\n",
    "print('随机森林模型训练完成')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de706756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机森林验证集AUC: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# 评估随机森林模型\n",
    "rf_auc = model_evaluation(rf_model, X_val, y_val)\n",
    "print(f'随机森林验证集AUC: {rf_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f63cb585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "开始模型融合...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================== 模型融合 ===================== #\n",
    "print('\\n='*80)\n",
    "print('开始模型融合...')\n",
    "print('='*80)\n",
    "\n",
    "# 方法1: 简单平均\n",
    "def simple_averaging(models, test):\n",
    "    print('计算简单平均预测...')\n",
    "    predictions = []\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict_proba(test)[:, 1]\n",
    "        predictions.append(pred)\n",
    "    return np.mean(np.array(predictions), axis=0)\n",
    "\n",
    "# 方法2: 加权平均 (根据验证集上的性能加权)\n",
    "def weighted_averaging(models, weights, test):\n",
    "    print('计算加权平均预测...')\n",
    "    predictions = []\n",
    "    for i, model in enumerate(models):\n",
    "        pred = model.predict_proba(test)[:, 1]\n",
    "        predictions.append(pred * weights[i])\n",
    "    return np.sum(np.array(predictions), axis=0) / np.sum(weights)\n",
    "\n",
    "# 方法3: Stacking (使用5折交叉验证生成训练集的meta特征)\n",
    "def stacking(base_models, meta_model, X, y, test, n_folds=5):\n",
    "    print('执行Stacking集成...')\n",
    "    # 生成元特征\n",
    "    meta_features_train = np.zeros((X.shape[0], len(base_models)))\n",
    "    meta_features_test = np.zeros((test.shape[0], len(base_models)))\n",
    "    \n",
    "    # K折交叉验证\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 对每个基础模型\n",
    "    for i, model in enumerate(base_models):\n",
    "        print(f'处理第 {i+1}/{len(base_models)} 个基础模型...')\n",
    "        # 对测试集的预测\n",
    "        try:\n",
    "            # XGBoost模型特殊处理 - 使用固定迭代次数而不使用早停\n",
    "            if isinstance(model, xgb.XGBClassifier):\n",
    "                print(\"  检测到XGBoost模型，使用固定迭代次数1000...\")\n",
    "                # 创建一个没有early_stopping的XGBoost模型副本\n",
    "                xgb_clone = xgb.XGBClassifier(\n",
    "                    objective='binary:logistic',\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=6,\n",
    "                    min_child_weight=3,\n",
    "                    gamma=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    scale_pos_weight=1,\n",
    "                    reg_alpha=0.1,\n",
    "                    reg_lambda=0.1,\n",
    "                    n_estimators=1000,  # 使用固定迭代次数1000\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                    verbosity=0\n",
    "                )\n",
    "                xgb_clone.fit(X, y)\n",
    "                meta_features_test[:, i] = xgb_clone.predict_proba(test)[:, 1]\n",
    "                \n",
    "                # 对训练集进行交叉验证预测\n",
    "                for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "                    print(f'  处理折 {fold+1}/{n_folds}')\n",
    "                    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train_fold = y.iloc[train_index]\n",
    "                    \n",
    "                    xgb_clone.fit(X_train_fold, y_train_fold)\n",
    "                    meta_features_train[val_index, i] = xgb_clone.predict_proba(X_val_fold)[:, 1]\n",
    "            else:\n",
    "                # 其他模型正常处理\n",
    "                model.fit(X, y)\n",
    "                meta_features_test[:, i] = model.predict_proba(test)[:, 1]\n",
    "                \n",
    "                # 对训练集进行交叉验证预测\n",
    "                for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "                    print(f'  处理折 {fold+1}/{n_folds}')\n",
    "                    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train_fold = y.iloc[train_index]\n",
    "                    \n",
    "                    model.fit(X_train_fold, y_train_fold)\n",
    "                    meta_features_train[val_index, i] = model.predict_proba(X_val_fold)[:, 1]\n",
    "        except Exception as e:\n",
    "            print(f\"模型 {i} 出现错误: {str(e)}\")\n",
    "            # 如果模型预测失败，使用该模型在验证集上的平均预测值填充\n",
    "            meta_features_train[:, i] = y.mean()\n",
    "            meta_features_test[:, i] = y.mean()\n",
    "    \n",
    "    # 训练元模型\n",
    "    print('训练元模型...')\n",
    "    meta_model.fit(meta_features_train, y)\n",
    "    \n",
    "    # 使用元模型预测测试集\n",
    "    final_predictions = meta_model.predict_proba(meta_features_test)[:, 1]\n",
    "    print('Stacking完成')\n",
    "    \n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "各模型AUC:\n",
      "LightGBM AUC: 0.7357\n",
      "XGBoost AUC: 0.7361\n",
      "CatBoost AUC: 0.7358\n",
      "RandomForest AUC: 0.7150\n"
     ]
    }
   ],
   "source": [
    "# 计算AUC\n",
    "models = [lgb_model, xgb_model, cat_model, rf_model]\n",
    "val_aucs = [lgb_auc, xgb_auc, cat_auc, rf_auc]\n",
    "# 打印每个模型的AUC\n",
    "print('\\n各模型AUC:')\n",
    "for model_name, auc in zip(['LightGBM', 'XGBoost', 'CatBoost', 'RandomForest'], val_aucs):\n",
    "    print(f'{model_name} AUC: {auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0abbe162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算简单平均预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "简单平均预测完成\n",
      "计算加权平均预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加权平均预测完成\n",
      "执行Stacking集成...\n",
      "处理第 1/4 个基础模型...\n",
      "  处理折 1/5\n",
      "  处理折 2/5\n",
      "  处理折 3/5\n",
      "  处理折 4/5\n",
      "  处理折 5/5\n",
      "处理第 2/4 个基础模型...\n",
      "  检测到XGBoost模型，使用固定迭代次数1000...\n",
      "  处理折 1/5\n",
      "  处理折 2/5\n",
      "  处理折 3/5\n",
      "  处理折 4/5\n",
      "  处理折 5/5\n",
      "处理第 3/4 个基础模型...\n",
      "0:\ttotal: 65.7ms\tremaining: 1m 5s\n",
      "100:\ttotal: 6.01s\tremaining: 53.5s\n",
      "200:\ttotal: 11.9s\tremaining: 47.4s\n",
      "300:\ttotal: 17.6s\tremaining: 40.8s\n",
      "400:\ttotal: 23.6s\tremaining: 35.3s\n",
      "500:\ttotal: 29.4s\tremaining: 29.3s\n",
      "600:\ttotal: 35.6s\tremaining: 23.7s\n",
      "700:\ttotal: 41.9s\tremaining: 17.9s\n",
      "800:\ttotal: 48.3s\tremaining: 12s\n",
      "900:\ttotal: 54.7s\tremaining: 6.01s\n",
      "999:\ttotal: 1m\tremaining: 0us\n",
      "  处理折 1/5\n",
      "0:\ttotal: 50.6ms\tremaining: 50.5s\n",
      "100:\ttotal: 5.22s\tremaining: 46.5s\n",
      "200:\ttotal: 10.2s\tremaining: 40.6s\n",
      "300:\ttotal: 14.9s\tremaining: 34.5s\n",
      "400:\ttotal: 19.3s\tremaining: 28.8s\n",
      "500:\ttotal: 24s\tremaining: 23.9s\n",
      "600:\ttotal: 28.9s\tremaining: 19.2s\n",
      "700:\ttotal: 33.5s\tremaining: 14.3s\n",
      "800:\ttotal: 38s\tremaining: 9.43s\n",
      "900:\ttotal: 42.6s\tremaining: 4.68s\n",
      "999:\ttotal: 47.6s\tremaining: 0us\n",
      "  处理折 2/5\n",
      "0:\ttotal: 55.6ms\tremaining: 55.5s\n",
      "100:\ttotal: 5.34s\tremaining: 47.5s\n",
      "200:\ttotal: 9.91s\tremaining: 39.4s\n",
      "300:\ttotal: 14.4s\tremaining: 33.5s\n",
      "400:\ttotal: 19s\tremaining: 28.4s\n",
      "500:\ttotal: 23.4s\tremaining: 23.3s\n",
      "600:\ttotal: 27.7s\tremaining: 18.4s\n",
      "700:\ttotal: 32.2s\tremaining: 13.7s\n",
      "800:\ttotal: 36.7s\tremaining: 9.12s\n",
      "900:\ttotal: 41.1s\tremaining: 4.52s\n",
      "999:\ttotal: 45.5s\tremaining: 0us\n",
      "  处理折 3/5\n",
      "0:\ttotal: 52.9ms\tremaining: 52.8s\n",
      "100:\ttotal: 4.71s\tremaining: 41.9s\n",
      "200:\ttotal: 9.09s\tremaining: 36.2s\n",
      "300:\ttotal: 13.5s\tremaining: 31.4s\n",
      "400:\ttotal: 17.9s\tremaining: 26.8s\n",
      "500:\ttotal: 22.3s\tremaining: 22.2s\n",
      "600:\ttotal: 26.7s\tremaining: 17.7s\n",
      "700:\ttotal: 31.4s\tremaining: 13.4s\n",
      "800:\ttotal: 35.8s\tremaining: 8.89s\n",
      "900:\ttotal: 40.2s\tremaining: 4.41s\n",
      "999:\ttotal: 44.5s\tremaining: 0us\n",
      "  处理折 4/5\n",
      "0:\ttotal: 50.6ms\tremaining: 50.5s\n",
      "100:\ttotal: 4.61s\tremaining: 41s\n",
      "200:\ttotal: 9.1s\tremaining: 36.2s\n",
      "300:\ttotal: 13.5s\tremaining: 31.4s\n",
      "400:\ttotal: 17.9s\tremaining: 26.8s\n",
      "500:\ttotal: 22.3s\tremaining: 22.2s\n",
      "600:\ttotal: 26.6s\tremaining: 17.7s\n",
      "700:\ttotal: 31s\tremaining: 13.2s\n",
      "800:\ttotal: 35.6s\tremaining: 8.85s\n",
      "900:\ttotal: 40s\tremaining: 4.39s\n",
      "999:\ttotal: 44.5s\tremaining: 0us\n",
      "  处理折 5/5\n",
      "0:\ttotal: 52ms\tremaining: 52s\n",
      "100:\ttotal: 4.59s\tremaining: 40.9s\n",
      "200:\ttotal: 8.91s\tremaining: 35.4s\n",
      "300:\ttotal: 13.4s\tremaining: 31.2s\n",
      "400:\ttotal: 17.9s\tremaining: 26.7s\n",
      "500:\ttotal: 22.2s\tremaining: 22.2s\n",
      "600:\ttotal: 26.6s\tremaining: 17.6s\n",
      "700:\ttotal: 30.9s\tremaining: 13.2s\n",
      "800:\ttotal: 35.3s\tremaining: 8.76s\n",
      "900:\ttotal: 39.6s\tremaining: 4.35s\n",
      "999:\ttotal: 43.8s\tremaining: 0us\n",
      "处理第 4/4 个基础模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   53.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   59.6s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  处理折 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   45.0s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  处理折 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   44.7s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  处理折 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   45.1s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  处理折 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   44.9s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  处理折 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   44.8s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练元模型...\n",
      "Stacking完成\n",
      "Stacking预测完成\n",
      "\n",
      "组合所有预测方法...\n",
      "最终预测组合完成\n"
     ]
    }
   ],
   "source": [
    "# 加权平均的权重 (根据模型在验证集上的表现)\n",
    "weights = val_aucs.copy()\n",
    "\n",
    "# 计算简单平均预测结果\n",
    "try:\n",
    "    simple_avg_pred = simple_averaging(models, test)\n",
    "    print('简单平均预测完成')\n",
    "except Exception as e:\n",
    "    print(f\"简单平均预测失败: {str(e)}\")\n",
    "    # 如果简单平均失败，使用表现最好的模型进行预测\n",
    "    best_model_idx = val_aucs.index(max(val_aucs))\n",
    "    simple_avg_pred = models[best_model_idx].predict_proba(test)[:, 1]\n",
    "    print(f'使用最佳模型 ({[\"LightGBM\", \"XGBoost\", \"CatBoost\", \"RandomForest\"][best_model_idx]}) 作为备选预测')\n",
    "\n",
    "# 使用加权平均进行预测\n",
    "try:\n",
    "    weighted_pred = weighted_averaging(models, weights, test)\n",
    "    print('加权平均预测完成')\n",
    "except Exception as e:\n",
    "    print(f\"加权平均预测失败: {str(e)}\")\n",
    "    # 如果加权平均失败，使用简单平均的结果\n",
    "    weighted_pred = simple_avg_pred\n",
    "    print('使用简单平均结果作为备选预测')\n",
    "\n",
    "# 使用Stacking进行预测\n",
    "try:\n",
    "    meta_model = LogisticRegression(C=1.0, max_iter=1000, random_state=42)\n",
    "    stacking_pred = stacking(models, meta_model, X_train, y_train, test)\n",
    "    print('Stacking预测完成')\n",
    "except Exception as e:\n",
    "    print(f\"Stacking预测失败: {str(e)}\")\n",
    "    # 如果Stacking失败，使用加权平均的结果\n",
    "    stacking_pred = weighted_pred\n",
    "    print('使用加权平均结果作为备选预测')\n",
    "\n",
    "# 最终融合策略 (加权平均多种融合方法)\n",
    "try:\n",
    "    print('\\n组合所有预测方法...')\n",
    "    final_pred = 0.20 * simple_avg_pred + 0.35 * weighted_pred + 0.45 * stacking_pred\n",
    "    print('最终预测组合完成')\n",
    "except Exception as e:\n",
    "    print(f\"最终融合失败: {str(e)}\")\n",
    "    # 如果最终融合失败，使用表现最好的融合方法结果\n",
    "    if max(val_aucs) == val_aucs[0]:  # LightGBM是最好的模型\n",
    "        final_pred = lgb_model.predict_proba(test)[:, 1]\n",
    "        print('使用LightGBM模型结果作为最终预测')\n",
    "    elif max(val_aucs) == val_aucs[1]:  # XGBoost是最好的模型\n",
    "        final_pred = xgb_model.predict_proba(test)[:, 1]\n",
    "        print('使用XGBoost模型结果作为最终预测')\n",
    "    elif max(val_aucs) == val_aucs[2]:  # CatBoost是最好的模型\n",
    "        final_pred = cat_model.predict_proba(test)[:, 1]\n",
    "        print('使用CatBoost模型结果作为最终预测')\n",
    "    else:  # RandomForest是最好的模型\n",
    "        final_pred = rf_model.predict_proba(test)[:, 1]\n",
    "        print('使用RandomForest模型结果作为最终预测')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b32d323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "=\n",
      "生成提交文件...\n",
      "================================================================================\n",
      "\n",
      "提交文件已生成!\n",
      "提交文件保存路径: c:\\Users\\16050\\Desktop\\submission_final.csv\n",
      "简单平均权重: 0.20, 加权平均权重: 0.35, Stacking权重: 0.45\n",
      "最终融合策略: 0.20 * 简单平均 + 0.35 * 加权平均 + 0.45 * Stacking\n"
     ]
    }
   ],
   "source": [
    "# ===================== 生成提交文件 ===================== #\n",
    "print('\\n='*80)\n",
    "print('生成提交文件...')\n",
    "print('='*80)\n",
    "submission['isDefault'] = final_pred\n",
    "submission.to_csv('submission_final.csv', index=False)\n",
    "\n",
    "print('\\n提交文件已生成!')\n",
    "print(f\"提交文件保存路径: {os.path.abspath('submission_final.csv')}\")\n",
    "# 打印各种融合方法的权重\n",
    "print(f'简单平均权重: 0.20, 加权平均权重: 0.35, Stacking权重: 0.45')\n",
    "print('最终融合策略: 0.20 * 简单平均 + 0.35 * 加权平均 + 0.45 * Stacking')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
