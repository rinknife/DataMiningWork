{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28a4f83",
   "metadata": {},
   "source": [
    "这是个非常实战性的问题，下面我来 **逐个分析你用的四个模型（LightGBM、XGBoost、CatBoost、RandomForest）各自的调参建议**，并结合你想要提升准确率（比如 AUC >= 0.78）给出系统的策略。\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 一、首先确认目标与指标\n",
    "\n",
    "> **你的目标是提升预测准确率/AUC 至 0.78 以上。**\n",
    "\n",
    "- 如果你说的是 **准确率**，那要注意类是否平衡；\n",
    "- 如果是 **AUC**，那是更推荐的指标，尤其在样本不平衡时。\n",
    "\n",
    "> **这里我默认你追求的是更高的 AUC，且你已经有验证集划分逻辑。**\n",
    "\n",
    "---\n",
    "\n",
    "## 🌳 二、四大模型调参建议（含推荐策略）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔥 1. LightGBM\n",
    "\n",
    "📌 当前核心参数：\n",
    "```python\n",
    "learning_rate=0.05, num_leaves=31, max_depth=7, min_child_samples=30\n",
    "```\n",
    "\n",
    "🎯 **调参建议：**\n",
    "\n",
    "| 调参维度 | 建议范围 | 说明 |\n",
    "|----------|----------|------|\n",
    "| `num_leaves` | 31 ~ 127 | 越大模型越复杂，但容易过拟合，搭配 max_depth 控制 |\n",
    "| `max_depth` | 6 ~ 10 | 控制树深度，防止过拟合 |\n",
    "| `min_child_samples` | 10 ~ 50 | 增大可以防止过拟合 |\n",
    "| `learning_rate` | 0.01 ~ 0.05 | 适当减小后搭配 `n_estimators` 增大，如 0.02 + 3000 |\n",
    "| `feature_fraction` | 0.6 ~ 0.9 | 每次训练用多少特征，防止过拟合 |\n",
    "| `bagging_fraction` | 0.6 ~ 0.9 | 每次训练用多少样本，搭配 bagging_freq |\n",
    "| `lambda_l1` / `lambda_l2` | 0.1 ~ 1.0 | 增加正则，尤其是过拟合时 |\n",
    "\n",
    "📌 **进阶建议：使用 `optuna` 自动调参库**（LightGBM 自带接口）。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔥 2. XGBoost\n",
    "\n",
    "📌 当前核心参数：\n",
    "```python\n",
    "learning_rate=0.05, max_depth=6, min_child_weight=3, gamma=0.1\n",
    "```\n",
    "\n",
    "🎯 **调参建议：**\n",
    "\n",
    "| 调参维度 | 建议范围 | 说明 |\n",
    "|----------|----------|------|\n",
    "| `max_depth` | 4 ~ 8 | 控制模型复杂度 |\n",
    "| `min_child_weight` | 1 ~ 10 | 控制叶子节点的最小样本权重，防止过拟合 |\n",
    "| `gamma` | 0 ~ 0.5 | 节点分裂所需的最小 loss reduction，越大越保守 |\n",
    "| `subsample` / `colsample_bytree` | 0.6 ~ 0.9 | 加强泛化能力 |\n",
    "| `reg_alpha`, `reg_lambda` | 0.1 ~ 2 | L1/L2 正则化项，限制复杂度 |\n",
    "| `scale_pos_weight` | 正负样本比例 | 若类别不平衡，设置该值会显著改善AUC |\n",
    "| `learning_rate + n_estimators` | eg. 0.01 + 3000 | 小学习率 + 大轮数是经典组合 |\n",
    "\n",
    "📌 Tip：XGBoost 更容易过拟合，小数据上应当正则更强。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔥 3. CatBoost\n",
    "\n",
    "📌 默认参数也能表现很好，但你可以用以下组合做进一步优化。\n",
    "\n",
    "🎯 **调参建议：**\n",
    "\n",
    "| 参数 | 范围 | 说明 |\n",
    "|------|------|------|\n",
    "| `iterations` | 1000 ~ 3000 | 提高迭代轮数以配合较小的学习率 |\n",
    "| `learning_rate` | 0.01 ~ 0.05 | 小学习率 + 大轮数效果好 |\n",
    "| `depth` | 5 ~ 10 | 决策树深度 |\n",
    "| `l2_leaf_reg` | 1 ~ 9 | L2正则化强度，调节过拟合 |\n",
    "| `bagging_temperature` | 0 ~ 1 | 控制随机性，0是无采样，1是完全随机 |\n",
    "| `random_strength` | 1 ~ 20 | 越高越不容易过拟合 |\n",
    "| `one_hot_max_size` | 2 ~ 10 | 控制何时用one-hot还是独热编码 |\n",
    "\n",
    "📌 CatBoost 的优势在于自动处理类别特征 + 不太敏感于缺失值填充。\n",
    "\n",
    "---\n",
    "\n",
    "### 🌲 4. RandomForest\n",
    "\n",
    "📌 随机森林往往在 baseline 阶段还不错，但效果上限较低。\n",
    "\n",
    "🎯 **调参建议：**\n",
    "\n",
    "| 参数 | 范围 | 说明 |\n",
    "|------|------|------|\n",
    "| `n_estimators` | 500 ~ 1500 | 增加树数量，提升稳定性 |\n",
    "| `max_depth` | 5 ~ 15 | 控制每棵树的最大深度 |\n",
    "| `min_samples_split` | 2 ~ 10 | 分裂所需最小样本数 |\n",
    "| `min_samples_leaf` | 1 ~ 10 | 叶节点最小样本数 |\n",
    "| `max_features` | sqrt, log2, float | 控制每棵树使用的特征数 |\n",
    "| `class_weight` | `'balanced'` | 若类别不平衡可开启，提升 Recall/AUC |\n",
    "\n",
    "📌 RF 相对稳定但泛化能力弱于 GBDT 类模型，可作为 stacking 基础模型。\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 三、通用策略让 AUC 上 0.78\n",
    "\n",
    "### ✅ 1. 模型集成是加分项（你已经做了👏）\n",
    "- 简单平均 → 加权平均 → stacking\n",
    "- 可继续加入更多 base model（如 L1 模型）\n",
    "\n",
    "### ✅ 2. 做特征选择/交互\n",
    "- 移除冗余特征（你已经有 correlation_selector）\n",
    "- 做多项式组合特征、目标编码、特征交互（比如乘法/除法组合）\n",
    "\n",
    "### ✅ 3. 用更高级的调参策略\n",
    "- 用 [`Optuna`](https://optuna.org/) 或 `GridSearchCV`/`RandomizedSearchCV` 做超参优化\n",
    "- CatBoost 和 LightGBM 都有原生集成 Optuna 接口\n",
    "\n",
    "### ✅ 4. 调整评估指标与标签构造\n",
    "- 检查标签是否有异常（如严重类不平衡）\n",
    "- 是否需要对样本做 undersampling/oversampling？\n",
    "- 分层抽样是否合理？是否需要更大验证集？\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 四、Bonus：调参建议总结表\n",
    "\n",
    "| 模型 | 核心需调 | 常见组合建议 |\n",
    "|------|----------|--------------|\n",
    "| LightGBM | `num_leaves + min_child_samples` | 小 `lr` + 正则强 |\n",
    "| XGBoost | `max_depth + min_child_weight + gamma` | 加权采样 + 正则化 |\n",
    "| CatBoost | `learning_rate + depth + l2_leaf_reg` | 默认也强，bagging 温度重要 |\n",
    "| RandomForest | `max_depth + min_samples_leaf` | 简单有效，适合做融合基础 |\n",
    "\n",
    "---\n",
    "\n",
    "需要我给你写个自动调参 `optuna` 的示例脚本吗？或者帮你画个模型 AUC 比较图？可以进一步让你知道哪个模型最值得投资优化～"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
